:batch-asciidoc: ./
:toc: left
:toclevels: 4

[[spring-batch-intro]]

== Spring Batch 介绍

在企业域环境中针对关键环境进行商业操作的时候，有许多应用程序需要进行批量处理。这些业务运营包括：

* 无需用户交互即可最有效地处理大量信息的自动化复杂处理。这些操作通常包括基于时间的事件 （例如，月末统计计算，通知或者消息通知）。
* 在非常大的数据集中重复处理复杂业务规则的定期应用（例如，保险利益确定或费率调整）。
* 整合从内部或者外部系统中收到的信息，这些信息通常要求格式，校验和事务范式处理到记录系统中。
批处理通常被用来针对企业每天产生超过亿万级别的数据量。

Spring Batch是一个轻量级的综合性批处理框架，可用于开发企业信息系统中那些至关重要的数据批量处理业务。
Spring Batch构建了人们期望的Spring Framework 特性（生产环境，基于 POJO 的开发和易于使用），
同时让开发者很容易的访问和使用企业级服务。Spring Batch 不是一个自动调度运行框架。在市面上已经有了很多企
业级和开源的自动运行框架（例如 Quartz，Tivoli， Control-M 等）。
Spring Batch 被设计与计划任务和调度程序一同协作完成任务，而不是替换调度程序。

Spring Batch 提供了可重用的功能，这些功能被用来对大量数据和记录进行处理，包括有日志/跟踪（logging/tracing），
事务管理（transaction management），作业处理状态（job processing statistics），作业重启（job restart），
作业跳过（job skip）和资源管理（resource management）。
此外还提供了许多高级服务和特性, 使之能够通过优化（optimization ） 和分片技术（partitioning techniques）
来实现极高容量和性能的批处理作业。
Spring Batch 能够简单（例如，将文件读入到数据库中或者运行一个存储过程）和复杂（例如，在数据库之间对海量数据进行移动或转换等）
情况下都能很好的工作。
可以在框架高度扩展的方式下执行大批量的作业来处理海量的信息。

[[springBatchBackground]]

=== 背景

在开源项目及其相关社区把大部分注意力集中在基于 web 和 微服务体系框架时框架中时，基于 Java 的批处理框架却无人问津，
尽管在企业 IT 环境中一直都有这种批处理的需求。但因为缺乏一个标准的、可重用的批处理框架导致在企业客户的 IT 系统中
存在着很多一次编写，一次使用的版本，以及很多不同的内部解决方案。

SpringSource (现被命名为 Pivotal) 和 Accenture（埃森哲）致力于通过合作来改善这种状况。
埃森哲在实现批处理架构上有着丰富的产业实践经验，SpringSource 有深入的技术开发积累，
背靠 Spring 框架提供的编程模型,意味着两者能够结合成为默契且强大的合作伙伴，创造出高质量的、市场认可的企业级 Java 解决方案，
填补这一重要的行业空白。两家公司都与许多通过开发基于Spring的批处理架构解决方案解决类似问题的客户合作。
这提供了一些有用的额外细节和实际约束，有助于确保解决方案可以应用于客户提出的现实问题。


埃森哲为Spring Batch项目贡献了以前专有的批处理体系结构框架，以及提供支持，增强功能和现有功能集的提交者资源。
埃森哲的贡献基于几十年来在使用最新几代平台构建批量架构方面的经验：COBOL / Mainframe，C / Unix以及现在的Java / Anywhere。

埃森哲与SpringSource之间的合作旨在促进软件处理方法，框架和工具的标准化，
在创建批处理应用程序时，企业用户可以始终如一地利用这些方法，框架和工具。希望为其企业IT环境提供标准的，
经过验证的解决方案的公司和政府机构可以从 Spring Batch 中受益。

[[springBatchUsageScenarios]]

=== 使用场景

一般的典型批处理程序：

* 从数据库，文件或队列中读取大量记录。
* 以某种方式处理数据。
* 以修改的形式写回数据。

Spring Batch自动执行此基本批处理迭代，提供处理类似事务的功能，通常在脱机环境中处理，无需任何用户交互。
批处理作业是大多数 IT 项目的一部分，Spring Batch 是唯一提供强大的企业级解决方案的开源框架。

业务场景

* 周期提交批处理任务
* 同时批处理进程：并非处理一个任务
* 分阶段的企业消息驱动处理
* 高并发批处理
* 失败后的手动或定时重启
* 按顺序处理任务依赖（使用工作流驱动的批处理插件）
* 部分处理：跳过记录（例如，回滚）
* 全批次事务：因为可能有小数据量的批处理或存在存储过程/脚本中

技术目标

* 批量的开发者使用 Spring 的编程模型：开发者能够更加专注于业务逻辑，让框架来解决基础的功能
* 在基础架构、批处理执行环境、批处理应用之间有明确的划分
* 以接口形式提供通用的核心服务，以便所有项目都能使用
* 提供简单的默认实现，以实现核心执行接口的“开箱即用”
* 通过在所有层中对 Spring 框架进行平衡配置，能够实现更加容易的配置，自定义和扩展服务。
* 所有存在的核心服务应该能够很容易的在不同系统架构层进行影响的情况进行替换或扩展。
* 提供一个简单的部署模块，使用 Maven 来进行编译的 JARs 架构，并与应用完全分离。

[[springBatchArchitecture]]
=== Spring Batch 体系结构
// TODO Make a separate document
Spring Batch 设计的时候充分考虑了可扩展性和各类最终用户。
下图显示了 Spring Batch 的架构层次示意图,这种架构层次为最终用户开发者提供了很好的扩展性与易用性。

.Spring 批量层级体系结构
image::{batch-asciidoc}images/spring-batch-layers.png[Figure 1.1: 批量层级体系结构, scaledwidth="60%"]

这个层级体系结构高亮显示了 Spring Batch 的 3 个主要组件：应用（Application），核心（Core）和 基础架构（Infrastructure）。
应用层包含了所有的批量作业和开发者使用 Spring Batch 写的所有自定义代码。批量核心层包含了所有运行和控制批量作业所需必要的运行时类。
同时还包括了有 `JobLauncher`, `Job`, 和 `Step`。应用层和核心层都构建在基础架构层之上。

基础架构层包含了有 读（readers）和 写（writers ）以及服务（services）。例如有针对服务使用， `RetryTemplate`。
基础架构层的这些东西，这些能够被应用层开发（readers 和 writers，
例如  `ItemReader` 和 `ItemWriter`）和批量核心框架（例如，retry，这个是核心层自己的库）所使用。

简单的来说，基础架构层为应用层和批量核心层提供了所需要的的基础内容，是整个  Spring Batch 的基础。我们针对 Spring Batch 的开发绝大部分情况是在应用层完成的。

[[batchArchitectureConsiderations]]
=== 一般批量处理的原则和使用指引

下面是一些关键的指导原则，可以在构批量处理解决方案可以参考。

* 请记住，通常批量处理体系结构将会影响在线应用的体系结构，同时反过来也是一样的。
在你为批量任务和在线应用进行设计架构和环境的时候请尽可能的使用公共的模块。

* 越简单越好，尽量在一个单独的批量应用中构建简单的批量处理，并避免复杂的逻辑结构。

* 尽量的保持存储的数据和进程存储在同一个地方（换句话说就是尽量将数据保存到你程序运行的地方）。

* 最小化系统资源的使用，尤其针对 I/O。尽量在内存中执行尽可能多的操作。

* 检查应用的 I/O（分析 SQL 语句）来避免不必要的的物理 I/O 使用。特别是以下四个常见的缺陷（flaws）需要避免：
** 在数据可以只读一次就可以缓存起来的情况下，针对每一个事务都来读取数据
** 多次读取/查询同一事务中已经读取过的数据
** 产生不必要的表格或者索引扫描
** 在 SQL 查询中不指定 WHERE 查询的值

* 在批量运行的时候不要将一件事情重复 2 次。例如，如果你需要针对你需要报表的数据汇总,请在处理每一条记录时使用增量来存储,
尽可能不要再去遍历一次同样的数据。

* 为批量进程在开始的时候就分配足够的内存，以避免在运行的时候再次分配内存。

* 总是将数据完整性假定为最坏情况。对数据进行适当的检查和数据校验以保持数据完整性（integrity）。

* 可能的话，请实现内部校验（checksums ）。例如，针对文本文件，应该有一条结尾记录，
这个记录将会说明文件中的总记录数和关键字段的集合（aggregate）。

* 尽可能早地在模拟生产环境下使用真实的数据量，以便于进行计划和执行压力测试。

* 在大数据量的批量中，数据备份可能会非常复杂和充满挑战，尤其是你的系统要求不间断（24 - 7）运行的系统。
数据库备份通常在设计时就考虑好了，但是文件备份也应该提升到同样的重要程度。如果系统依赖于文本文件，
文件备份程序不仅要正确设置和形成文档，还要定期进行测试。

[[batchProcessingStrategy]]
=== 批量处理策略

为了帮助设计和实现批量处理系统，基本的批量应用是通过块和模式来构建的，
同时也应该能够为程序开发人员和设计人员提供结构的样例和基础的批量处理程序。

当你开始设计一个批量作业任务的时候，商业逻辑应该被拆分一系列的步骤，而这些步骤又是可以通过下面的标准构件块来实现的：

* __转换应用程序（Conversion Applications）：__ 针对每一个从外部系统导出或者提供的各种类型的文件，
我们都需要创建一个转换应用程序来讲这些类型的文件和数据转换为处理所需要的标准格式。
这个类型的批量应用程序可以是正规转换工具模块中的一部分，也可以是整个的转换工具模块（请查看：基本的批量服务（Basic Batch Services））。
// TODO Add a link to "Basic Batch Services", once you discover where that content is.
* __校验应用程序（Validation Applications）：__ 校验应用程序能够保证所有的输入和输出记录都是正确和一致的。
校验通常是基于头和尾进行校验的，校验码和校验算法通常是针对记录的交叉验证。
* __提取应用（Extract Applications）：__ 这个应用程序通常被用来从数据库或者文本文件中读取一系列的记录，
并对记录的选择通常是基于预先确定的规则，然后将这些记录输出到输出文件中。
* __提取/更新应用（Extract/Update Applications）：__ 这个应用程序通常被用来从数据库或者文本文件中读取记录，
并将每一条读取的输入记录更新到数据库或者输出数据库中。
* __处理和更新应用（Processing and Updating Applications）：__ 这种程序对从提取或验证程序 传过来的输入事务记录进行处理。
这处理通常包括有读取数据库并且获得需要处理的数据，为输出处理更新数据库或创建记录。
* __输出和格式化应用（Output/Format Applications）：__ 一个应用通过读取一个输入文件，
对输入文件的结构重新格式化为需要的标准格式，然后创建一个打印的输出文件，或将数据传输到其他的程序或者系统中。

更多的，一个基本的应用外壳应该也能够被针对商业逻辑来提供，这个外壳通常不能通过上面介绍的这些标准模块来完成。
// TODO What is an example of such a system?

另外的一个主要的构建块，每一个引用通常可以使用下面的一个或者多个标准工具步骤，例如：


* 分类（Sort）: 一个程序可以读取输入文件后生成一个输出文件，在这个输出文件中可以对记录进行重新排序，
重新排序的是根据给定记录的关键字段进行重新排序的。分类通常使用标准的系统工具来执行。
* 拆分（Split）：一个程序可以读取输入文件后，根据需要的字段值，将输入的文件拆分为多个文件进行输出。拆分通常使用标准的系统工具来执行。
* 合并（Merge）：一个程序可以读取多个输入文件，然后将多个输入文件进行合并处理后生成为一个单一的输出文件。
合并可以自定义或者由参数驱动的（parameter-driven）系统实用程序来执行。

批量处理应用程序可以通过下面的输入数据类型来进行分类：

* 数据库驱动应用程序（Database-driven applications）可以通过从数据库中获得的行或值来进行驱动。
* 文件驱动应用程序（File-driven applications） 可以通过从文件中获得的数据来进行驱动。
* 消息驱动应用程序（Message-driven applications） 可以通过从消息队列中获得的数据来进行驱动。

所有批量处理系统的处理基础都是策略（strategy）。对处理策略进行选择产生影响的因素包括有：预估批量处理需要处理的数据量，
在线并发量，和另外一个批量处理系统的在线并发量，
可用的批量处理时间窗口（很多企业都希望系统是能够不间断运行的，基本上来说批量处理可能没有处理时间窗口）。

针对批量处理的标准处理选项包括有（按实现复杂度的递增顺序）：

* 在一个批处理窗口中执行常规离线批处理
* 并发批量 / 在线处理
* 并发处理很多不同的批量处理或者有很多批量作业在同一时间运行
* 分区（Partitioning），就是在同一时间有很多示例在运行相同的批量作业
* 混合上面的一些需求

上面的一些选项或者所有选项能够被商业的任务调度所支持。

在下面的部分，我们将会针对上面的处理选项来对细节进行更多的说明。需要特别注意的是，
批量处理程序使用提交和锁定策略将会根据批量处理的不同而有所不同。作为最佳实践，在线锁策略应该使用相同的原则。
因此，在设计批处理整体架构时不能简单地拍脑袋决定，需要进行详细的分析和论证。

锁定策略可以仅仅使用常见的数据库锁或者你也可以在系统架构中使用其他的自定义锁定服务。
这个锁服务将会跟踪数据库的锁（例如在一个专用的数据库表（db-table）中存储必要的信息），然后在应用程序请求数据库操作时授予权限或拒绝。
重试逻辑应该也需要在系统架构中实现，以避免批量作业中的因资源锁定而导致批量任务被终止。

*1. Normal processing in a batch window* For simple batch processes running in a separate
batch window where the data being updated is not required by on-line users or other batch
processes, concurrency is not an issue and a single commit can be done at the end of the
batch run.

In most cases, a more robust approach is more appropriate. Keep in mind that batch
systems have a tendency to grow as time goes by, both in terms of complexity and the data
volumes they handle. If no locking strategy is in place and the system still relies on a
single commit point, modifying the batch programs can be painful. Therefore, even with
the simplest batch systems, consider the need for commit logic for restart-recovery
options as well as the information concerning the more complex cases described later in
this section.

*2. Concurrent batch or on-line processing* Batch applications processing data that can
be simultaneously updated by on-line users should not lock any data (either in the
database or in files) which could be required by on-line users for more than a few
seconds. Also, updates should be committed to the database at the end of every few
transactions. This minimizes the portion of data that is unavailable to other processes
and the elapsed time the data is unavailable.

Another option to minimize physical locking is to have logical row-level locking
implemented with either an Optimistic Locking Pattern or a Pessimistic Locking Pattern.


* Optimistic locking assumes a low likelihood of record contention. It typically means
inserting a timestamp column in each database table used concurrently by both batch and
on-line processing. When an application fetches a row for processing, it also fetches the
timestamp. As the application then tries to update the processed row, the update uses the
original timestamp in the WHERE clause. If the timestamp matches, the data and the
timestamp are updated. If the timestamp does not match, this indicates that another
application has updated the same row between the fetch and the update attempt. Therefore,
the update cannot be performed.


* Pessimistic locking is any locking strategy that assumes there is a high likelihood of
record contention and therefore either a physical or logical lock needs to be obtained at
retrieval time. One type of pessimistic logical locking uses a dedicated lock-column in
the database table. When an application retrieves the row for update, it sets a flag in
the lock column. With the flag in place, other applications attempting to retrieve the
same row logically fail. When the application that sets the flag updates the row, it also
clears the flag, enabling the row to be retrieved by other applications. Please note that
the integrity of data must be maintained also between the initial fetch and the setting
of the flag, for example by using db locks (such as `SELECT FOR UPDATE`). Note also that
this method suffers from the same downside as physical locking except that it is somewhat
easier to manage building a time-out mechanism that gets the lock released if the user
goes to lunch while the record is locked.

These patterns are not necessarily suitable for batch processing, but they might be used
for concurrent batch and on-line processing (such as in cases where the database does not
support row-level locking). As a general rule, optimistic locking is more suitable for
on-line applications, while pessimistic locking is more suitable for batch applications.
Whenever logical locking is used, the same scheme must be used for all applications
accessing data entities protected by logical locks.

Note that both of these solutions only address locking a single record. Often, we may
need to lock a logically related group of records. With physical locks, you have to
manage these very carefully in order to avoid potential deadlocks. With logical locks, it
is usually best to build a logical lock manager that understands the logical record
groups you want to protect and that can ensure that locks are coherent and
non-deadlocking. This logical lock manager usually uses its own tables for lock
management, contention reporting, time-out mechanism, and other concerns.

*3. Parallel Processing* Parallel processing allows multiple batch runs or jobs to run in
parallel to minimize the total elapsed batch processing time. This is not a problem as
long as the jobs are not sharing the same files, db-tables, or index spaces. If they do,
this service should be implemented using partitioned data. Another option is to build an
architecture module for maintaining interdependencies by using a control table. A control
table should contain a row for each shared resource and whether it is in use by an
application or not. The batch architecture or the application in a parallel job would
then retrieve information from that table to determine if it can get access to the
resource it needs or not.

If the data access is not a problem, parallel processing can be implemented through the
use of additional threads to process in parallel.  In the mainframe environment, parallel
job classes have traditionally been used, in order to ensure adequate CPU time for all
the processes. Regardless, the solution has to be robust enough to ensure time slices for
all the running processes.

Other key issues in parallel processing include load balancing and the availability of
general system resources such as files, database buffer pools, and so on. Also note that
the control table itself can easily become a critical resource.

*4. Partitioning* Using partitioning allows multiple versions of large batch applications
to run concurrently. The purpose of this is to reduce the elapsed time required to
process long batch jobs. Processes that can be successfully partitioned are those where
the input file can be split and/or the main database tables partitioned to allow the
application to run against different sets of data.

In addition, processes which are partitioned must be designed to only process their
assigned data set. A partitioning architecture has to be closely tied to the database
design and the database partitioning strategy. Note that database partitioning does not
necessarily mean physical partitioning of the database, although in most cases this is
advisable. The following picture illustrates the partitioning approach:

.Partitioned Process
image::{batch-asciidoc}images/partitioned.png[Figure 1.2: Partitioned Process, scaledwidth="60%"]


The architecture should be flexible enough to allow dynamic configuration of the number
of partitions. Both automatic and user controlled configuration should be considered.
Automatic configuration may be based on parameters such as the input file size and the
number of input records.

*4.1 Partitioning Approaches* Selecting a partitioning approach has to be done on a
case-by-case basis. The  following list describes some of the possible partitioning
approaches:

_1. Fixed and Even Break-Up of Record Set_

This involves breaking the input record set into an even number of portions (for example,
10, where each portion has exactly 1/10th of the entire record set). Each portion is then
processed by one instance of the batch/extract application.

In order to use this approach, preprocessing is required to split the recordset up. The
result of this split will be a lower and upper bound placement number which can be used
as input to the batch/extract application in order to restrict its processing to only its
portion.

Preprocessing could be a large overhead, as it has to calculate and determine the bounds
of each portion of the record set.

_2. Break up by a Key Column_

This involves breaking up the input record set by a key column, such as a location code,
and assigning data from each key to a batch instance. In order to achieve this, column
values can be either:

* Assigned to a batch instance by a partitioning table (described later in this
section).

* Assigned to a batch instance by a portion of the value (such as 0000-0999, 1000 - 1999,
and so on).

Under option 1, adding new values means a manual reconfiguration of the batch/extract to
ensure that the new value is added to a particular instance.

Under option 2, this ensures that all values are covered via an instance of the batch
job. However, the number of values processed by one instance is dependent on the
distribution of column values (there may be a large number of locations in the 0000-0999
range, and few in the 1000-1999 range). Under this option, the data range should be
designed with partitioning in mind.

Under both options, the optimal even distribution of records to batch instances cannot be
realized. There is no dynamic configuration of the number of batch instances used.

_3. Breakup by Views_

This approach is basically breakup by a key column but on the database level. It involves
breaking up the recordset into views. These views are used by each instance of the batch
application during its processing. The breakup is done by grouping the data.

With this option, each instance of a batch application has to be configured to hit a
particular view (instead of the master table). Also, with the addition of new data
values, this new group of data has to be included into a view. There is no dynamic
configuration capability, as a change in the number of instances results in a change to
the views.

_4. Addition of a Processing Indicator_

This involves the addition of a new column to the input table, which acts as an
indicator. As a preprocessing step, all indicators are marked as being non-processed.
During the record fetch stage of the batch application, records are read on the condition
that that record is marked as being non-processed, and once they are read (with lock),
they are marked as being in processing. When that record is completed, the indicator is
updated to either complete or error. Many instances of a batch application can be started
without a change, as the additional column ensures that a record is only processed once.
// TODO On completion, what is the record marked as? Same for on error. (I expected a
sentence or two on the order of "On completion, indicators are marked as being
complete.")

With this option, I/O on the table increases dynamically. In the case of an updating
batch application, this impact is reduced, as a write must occur anyway.

_5. Extract Table to a Flat File_

This involves the extraction of the table into a file. This file can then be split into
multiple segments and used as input to the batch instances.

With this option, the additional overhead of extracting the table into a file and
splitting it may cancel out the effect of multi-partitioning. Dynamic configuration can
be achieved by changing the file splitting script.

_6. Use of a Hashing Column_

This scheme involves the addition of a hash column (key/index) to the database tables
used to retrieve the driver record. This hash column has an indicator to determine which
instance of the batch application processes this particular row. For example, if there
are three batch instances to be started, then an indicator of 'A' marks a row for
processing by instance 1, an indicator of 'B' marks a row for processing by instance 2,
and an indicator of 'C' marks a row for processing by instance 3.

The procedure used to retrieve the records would then have an additional `WHERE` clause
to select all rows marked by a particular indicator. The inserts in this table would
involve the addition of the marker field, which would be defaulted to one of the
instances (such as 'A').

A simple batch application would be used to update the indicators, such as to
redistribute the load between the different instances. When a sufficiently large number
of new rows have been added, this batch can be run (anytime, except in the batch window)
to redistribute the new rows to other instances.
// TODO Why not in the batch window?

Additional instances of the batch application only require the running of the batch
application as described in the preceding paragraphs to redistribute the indicators to
work with a new number of instances.

*4.2 Database and Application Design Principles*

An architecture that supports multi-partitioned applications which run against
partitioned database tables using the key column approach should include a central
partition repository for storing partition parameters. This provides flexibility and
ensures maintainability. The repository generally consists of a single table, known as
the partition table.

Information stored in the partition table is static and, in general, should be maintained
by the DBA. The table should consist of one row of information for each partition of a
multi-partitioned application. The table should have columns for  Program ID Code,
Partition Number (logical ID of the partition), Low Value of the db key column for this
partition, and High Value of the db key column for this partition.

On program start-up, the program `id` and partition number should be passed to the
application from the architecture (specifically, from the Control Processing Tasklet). If
a key column approach is used, these variables are used to read the partition table in
order to determine what range of data the application is to process. In addition the
partition number must be used throughout the processing to:

* Add to the output files/database updates in order for the merge process to work
properly.
* Report normal processing to the batch log and any errors to the architecture error
handler.

*4.3 Minimizing Deadlocks*

When applications run in parallel or are partitioned, contention in database resources
and deadlocks may occur. It is critical that the database design team eliminates
potential contention situations as much as possible as part of the database design.

Also, the developers must ensure that the database index tables are designed with
deadlock prevention and performance in mind.

Deadlocks or hot spots often occur in administration or architecture tables, such as log
tables, control tables, and lock tables. The implications of these should be taken into
account as well. A realistic stress test is crucial for identifying the possible
bottlenecks in the architecture.

To minimize the impact of conflicts on data, the architecture should provide services
such as wait-and-retry intervals when attaching to a database or when encountering a
deadlock. This means a built-in mechanism to react to certain database return codes and,
instead of issuing an immediate error, waiting a predetermined amount of time and
retrying the database operation.

*4.4 Parameter Passing and Validation*

The partition architecture should be relatively transparent to application developers.
The architecture should perform all tasks associated with running the application in a
partitioned mode, including:

* Retrieving partition parameters before application start-up.
* Validating partition parameters before application start-up.
* Passing parameters to the application at start-up.

The validation should include checks to ensure that:

* The application has sufficient partitions to cover the whole data range.
* There are no gaps between partitions.

If the database is partitioned, some additional validation may be necessary to ensure
that a single partition does not span database partitions.

Also, the architecture should take into consideration the consolidation of partitions.
Key questions include:

* Must all the partitions be finished before going into the next job step?
* What happens if one of the partitions aborts?
